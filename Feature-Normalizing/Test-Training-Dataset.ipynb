{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and test sets\n",
        "\n",
        "We've seen previously how to fit a model to a dataset. In this exercise, we'll be looking at how to check and confirm the validity and performance of our models by using training and testing sets.\n",
        "As usual, we begin by loading in and having a look at our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "!pip install statsmodels\n",
        "\n",
        "data = pandas.read_csv(\"dog-training.csv\", delimiter=\"\\t\")\n",
        "\n",
        "print(data.shape)\n",
        "print(data.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're interested in the relationship between a dog's weight and the amount of rescues it performed in the previous year. Let's begin by plotting `rescues_last_year` as a function of `weight_last_year`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import graphing\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# First, we define our formula using a special syntax\n",
        "# This says that rescues_last_year is explained by weight_last_year\n",
        "formula = \"rescues_last_year ~ weight_last_year\"\n",
        "\n",
        "model = smf.ols(formula = formula, data = data).fit()\n",
        "\n",
        "graphing.scatter_2D(data, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There seems to be a pretty clear relationship between a dog's weight and the number of rescues it's performed. \n",
        "That seems pretty reasonable, as we'd expect heavier dogs to be bigger and stronger and thus better at saving lives!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train/test split\n",
        "\n",
        "This time, instead of fitting a model to the entirety of our dataset, we're going to separate our dataset into two smaller partitions: a _training set_ and a _test set_.\n",
        "\n",
        "The _training set_ is the largest of the two, usually made up of between 70-80% of the overall dataset, with the rest of the dataset making up the _test set_.\n",
        "\n",
        "By splitting our data, we're able to gauge the performance of our model when confronted with previously unseen data. \n",
        "\n",
        "Notice that data on the _test set_ is never used in training. For that reason, it's commonly referred to as *unseen data* or data that is *unknown by the model*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Obtain the label and feature from the original data\n",
        "dataset = data[['rescues_last_year','weight_last_year']]\n",
        "\n",
        "# Split the dataset in an 70/30 train/test ratio. We also obtain the respective corresponding indices from the original dataset.\n",
        "train, test = train_test_split(dataset, train_size=0.7, random_state=21)\n",
        "\n",
        "print(\"Train\")\n",
        "print(train.head())\n",
        "print(train.shape)\n",
        "\n",
        "print(\"Test\")\n",
        "print(test.head())\n",
        "print(test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice that these sets are different, and that the _training set_ and _test set_ contain 70% and 30% of the overall data, respectively.\n",
        "\n",
        "Let's have a look at how the _training set_ and _test set_ are separated out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You don't need to understand this code well\n",
        "# It's just used to create a scatter plot\n",
        "\n",
        "# concatenate training and test so they can be graphed\n",
        "plot_set = pandas.concat([train,test])\n",
        "plot_set[\"Dataset\"] = [\"train\"] * len(train) + [\"test\"] * len(test)\n",
        "\n",
        "# Create graph\n",
        "graphing.scatter_2D(plot_set, \"weight_last_year\", \"rescues_last_year\", \"Dataset\", trendline = lambda x: model.params[1] * x + model.params[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Set\n",
        "\n",
        "We begin by training our model using the _training set_, testing its performance with the same _training set_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# First, we define our formula using a special syntax\n",
        "# This says that rescues_last_year is explained by weight_last_year\n",
        "formula = \"rescues_last_year ~ weight_last_year\"\n",
        "\n",
        "# Create and train the model\n",
        "model = smf.ols(formula = formula, data = train).fit()\n",
        "\n",
        "# Graph the result against the data\n",
        "graphing.scatter_2D(train, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can gauge our model's performance by calculating the _mean squared error_ (MSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We use the in-buit sklearn function to calculate the MSE\n",
        "correct_labels = train['rescues_last_year']\n",
        "predicted = model.predict(train['weight_last_year'])\n",
        "\n",
        "MSE = mse(correct_labels, predicted)\n",
        "print('MSE = %f ' % MSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Set\n",
        "\n",
        "Next, we test the same model's performance using the _test set_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graphing.scatter_2D(test, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a look at the MSE again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct_labels = test['rescues_last_year']\n",
        "predicted = model.predict(test['weight_last_year'])\n",
        "\n",
        "MSE = mse(correct_labels, predicted)\n",
        "print('MSE = %f ' % MSE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We learn that the model performs much better on the known _training data_ than on the unseen _test data_ (remember that higher MSE values are worse).  \n",
        "\n",
        "The reason can be due to a number of factors, but first and foremost is _overfitting_, which is when a model matches the data in the _training set_ too closely. This means that it will perform very well on the _training set_, but will not _generalize_ well. (that is, it won't work well with other datasets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New Dataset\n",
        "\n",
        "To illustrate our point further, let's have a look at how our model performs when confronted with a completely new, unseen, and larger dataset. For our scenario, we'll use data provided by the avalanche rescue charity's European branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load an alternative dataset from the charity's European branch\n",
        "new_data = pandas.read_csv(\"dog-training-switzerland.csv\", delimiter=\"\\t\")\n",
        "\n",
        "print(new_data.shape)\n",
        "new_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The features are the same, but we have much more data this time. Let's see how our model does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the fitted model against this new dataset. \n",
        "\n",
        "graphing.scatter_2D(new_data, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, the MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct_labels = new_data['rescues_last_year']\n",
        "predicted = model.predict(new_data['weight_last_year'])\n",
        "\n",
        "MSE = mse(correct_labels, predicted)\n",
        "print('MSE = %f ' % MSE)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-azureml_py38-py"
    },
    "kernelspec": {
      "display_name": "azureml_py38",
      "language": "python",
      "name": "conda-env-azureml_py38-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
