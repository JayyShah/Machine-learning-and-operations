{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test sets in depth\n",
        "\n",
        "In the previous exercise, we looked at how to split our data into training and test sets to evaluate model performance.\n",
        "\n",
        "We'll now repeat the last exercise, but this time we'll look at what happens when we split the data in different ways and ratios.\n",
        "\n",
        "First, let's recall what's in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "!pip install statsmodels\n",
        "import graphing\n",
        "\n",
        "data = pandas.read_csv(\"dog-training.csv\", delimiter=\"\\t\")\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a quick look at what the distribution of our data looks like (remember that we were using `weight_last_year` to predict the value of `rescues_last_year`). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtain the label and feature from the original data\n",
        "dataset = data[['rescues_last_year','weight_last_year']]\n",
        "\n",
        "# Print the number of observations\n",
        "print(\"No. observations:\", dataset.shape[0])\n",
        "\n",
        "# Graph the distribution of variables for the unsplit dataset\n",
        "graphing.scatter_2D(dataset, 'rescues_last_year', 'weight_last_year')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice we have 50 observations plotted (which corresponds to the number of samples in the dataset).\n",
        "\n",
        "## Dataset split ratio comparison\n",
        "\n",
        "We'll now split our dataset using different ratios to understand how these can affect our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split Dataset using different ratios 50:50, 60:40, 70:30, 80:20\n",
        "train_5050, test_5050 = train_test_split(dataset, test_size=0.5, random_state=2)\n",
        "train_6040, test_6040 = train_test_split(dataset, test_size=0.4, random_state=2)\n",
        "train_7030, test_7030 = train_test_split(dataset, test_size=0.3, random_state=2)\n",
        "train_8020, test_8020 = train_test_split(dataset, test_size=0.2, random_state=2)\n",
        "\n",
        "# Add a column to each set to identify if a datapoint belongs to \"train\" or \"test\"\n",
        "train_5050, test_5050 = train_5050.assign(Set=\"train\"), test_5050.assign(Set=\"test\")\n",
        "train_6040, test_6040 = train_6040.assign(Set=\"train\"), test_6040.assign(Set=\"test\")\n",
        "train_7030, test_7030 = train_7030.assign(Set=\"train\"), test_7030.assign(Set=\"test\")\n",
        "train_8020, test_8020 = train_8020.assign(Set=\"train\"), test_8020.assign(Set=\"test\")\n",
        "\n",
        "# Concatenate the \"train\" and \"test\" sets for each split so we can plot them on the same chart\n",
        "df_5050 = pandas.concat([train_5050, test_5050], axis=0)\n",
        "df_6040 = pandas.concat([train_6040, test_6040], axis=0)\n",
        "df_7030 = pandas.concat([train_7030, test_7030], axis=0)\n",
        "df_8020 = pandas.concat([train_8020, test_8020], axis=0)\n",
        "\n",
        "# Plot each distribution for comparison\n",
        "graphing.scatter_2D(df_5050, \"weight_last_year\", \"rescues_last_year\", title=\"50:50 split\", label_colour=\"Set\", show=True)\n",
        "graphing.scatter_2D(df_6040, \"weight_last_year\", \"rescues_last_year\", title=\"60:40 split\", label_colour=\"Set\", show=True)\n",
        "graphing.scatter_2D(df_7030, \"weight_last_year\", \"rescues_last_year\", title=\"70:30 split\", label_colour=\"Set\", show=True)\n",
        "graphing.scatter_2D(df_8020, \"weight_last_year\", \"rescues_last_year\", title=\"80:20 split\", label_colour=\"Set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice on how the first splits leave us with relatively small _training_ datasets. On the 50:50 split, we have only 25 samples to train with.\n",
        "\n",
        "On the other hand, the latter ones leave us much more data for training, but relatively little for testing. The 80:20 split leaves us with only 10 samples in the _test_ set!\n",
        "\n",
        "\n",
        "Let's take a look at the distributions of _training_ data for each split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a column for each \"train\" set that identifies the split used\n",
        "train_8020 = train_8020.assign(Split=\"80:20\")\n",
        "train_7030 = train_7030.assign(Split=\"70:30\")\n",
        "train_6040 = train_6040.assign(Split=\"60:40\")\n",
        "train_5050 = train_5050.assign(Split=\"50:50\")\n",
        "\n",
        "# Concatenate training sets so we can plot them on the same chart\n",
        "split_df = pandas.concat([train_5050, train_6040, train_7030, train_8020], axis=0)\n",
        "\n",
        " # Plot a histogram of data distribution\n",
        "graphing.multiple_histogram(split_df, label_x=\"rescues_last_year\", label_group=\"Split\", title=\"Distribution of Training data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can draw a couple of conclusions from the plot above:\n",
        "\n",
        "1. The `train_test_split()` function we used does a fairly good job of keeping a fair distribution of data across different ratios. It attempts to keep different values represented in the same proportion.\n",
        "\n",
        "2. When we use a `50:50` ratio, however, we have to leave so much data out of the _train_ set that some values aren't present anymore! (Can you spot where blue bars are missing?)\n",
        "\n",
        "Point `2` is especially concerning because if that data isn't available for training, our model can't learn from it, and therefore won't make accurate predictions. In other words, using a `50:50` split doesn't look like a good idea for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting and evaluating models with different split ratios\n",
        "\n",
        "Let's fit models using different splits, then see how they appear to perform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "def train_and_test_model(name, train, test):\n",
        "    '''\n",
        "    This method creates a model, trains it on the provided data, and assesses \n",
        "    it against the test data\n",
        "    '''\n",
        "    # This formula says that rescues_last_year is explained by weight_last_year\n",
        "    formula = \"rescues_last_year ~ weight_last_year\"\n",
        "\n",
        "    # Create and train a linear regression model using the training data\n",
        "    model = smf.ols(formula = formula, data = train).fit()\n",
        "\n",
        "    # Now evaluate the model (by calculating the Mean Squared Error) using the \n",
        "    # corresponding \"test\" set for that split\n",
        "    correct_answers = test['rescues_last_year']\n",
        "    predictions = model.predict(test['weight_last_year'])\n",
        "    MSE = mse(correct_answers, predictions)\n",
        "    print(name + ' MSE = %f ' % MSE)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train a model and test it for each kind of split\n",
        "print(\"Mean Squared Error values (smaller is better)\")\n",
        "model_5050 = train_and_test_model(\"50:50\", train_5050, test_5050)\n",
        "model_6040 = train_and_test_model(\"60:40\", train_6040, test_6040)\n",
        "model_7030 = train_and_test_model(\"70:30\", train_7030, test_7030)\n",
        "model_8020 = train_and_test_model(\"80:20\", train_8020, test_8020)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preceding code trains one model for each ratio, using the corresponding _train_ set for that ratio. It then calculates the MSE (Mean Squared Error) for each model, using its corresponding _test_ set.\n",
        "\n",
        "The results seem mixed. The `80:20` ratio was best, but there's no clear pattern as to whether growing or shrinking the training set is helpful.\n",
        "\n",
        "Two things are influencing our results. First, the larger the _test_ set, the more we can trust the test results. Secondly, models usually will train better with larger training sets. \n",
        "\n",
        "These influences are at odds with one another, and how influential they are is exaggerated here because our dataset it very small. In this particular situation, it's hard to assess whether the `60:40` split is truly better than the `70:30` split, for example. This is because the `70:30 split` might just give the appearance of being worse because it was tested against a less-representative (smaller) test set.\n",
        "\n",
        "## Model evaluation\n",
        "\n",
        "Let's take a look now at what happens when these models are used against a much larger dataset on which it wasn't trained or tested. This can happen in the real world because we choose to hold back data in the beginning, or simply because we collect data after training our model. In our current scenario, this is new data given to us by our avalanche-rescue charity's European arm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# Load some dog statistics from our charity's European arm\n",
        "swiss_data = pandas.read_csv(\"dog-training-switzerland.csv\", delimiter=\"\\t\")\n",
        "\n",
        "# Show show information about the data\n",
        "print(f\"The Swiss dataset contains {swiss_data.shape[0]} samples\")\n",
        "graphing.scatter_2D(swiss_data, 'rescues_last_year', 'weight_last_year')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is clearly a much larger sample of data. Let's see how our models perform. Note that we aren't retraining the models here; we're simply using them to make predictions on a new dataset to assess how well they perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test our models against the swiss data\n",
        "features = swiss_data['weight_last_year']\n",
        "correct_answers = swiss_data['rescues_last_year']\n",
        "\n",
        "# We will now assess our models. Notice we're not training them again.\n",
        "# We are simply testing them against some new data \n",
        "\n",
        "# Assess the model trained on a 50:50 split\n",
        "predictions = model_5050.predict(features)\n",
        "MSE = mse(correct_answers, predictions)\n",
        "print('50:50 MSE = %f ' % MSE)\n",
        "\n",
        "# Assess the model trained on a 60:40 split\n",
        "predictions = model_6040.predict(features)\n",
        "MSE = mse(correct_answers, predictions)\n",
        "print('60:40 MSE = %f ' % MSE)\n",
        "\n",
        "# Assess the model trained on a 70:30 split\n",
        "predictions = model_7030.predict(features)\n",
        "MSE = mse(correct_answers, predictions)\n",
        "print('70:30 MSE = %f ' % MSE)\n",
        "\n",
        "# Assess the model trained on a 80:20 split\n",
        "predictions = model_8020.predict(features)\n",
        "MSE = mse(correct_answers, predictions)\n",
        "print('80:20 MSE = %f ' % MSE)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-azureml_py38-py"
    },
    "kernelspec": {
      "display_name": "azureml_py38",
      "language": "python",
      "name": "conda-env-azureml_py38-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
