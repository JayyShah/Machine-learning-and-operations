{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Why Computational Graphs?"
      ],
      "metadata": {
        "id": "IGzBSo7H6xKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When talking about neural networks in any context, backpropagation is an important topic to understand because it is the algorithm used for training deep neural networks.\n",
        "\n",
        "Backpropagation is used to calculate derivatives which is what you need to keep optimizing parameters of the model and allowing the model to learn on the task at hand.\n",
        "\n",
        "Many of the deep learning frameworks today like PyTorch does the backpropagation out-of-the-box using [**automatic differentiation**](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).\n",
        "\n",
        "To better understand how this is done it's important to talk about **computational graphs** which defines the flow of computations that are carried out throughout the network. Along the way we will use `torch.autograd` to demonstrate in code how this works.  "
      ],
      "metadata": {
        "id": "lkFMbiPDrGIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not a neural network of any sort.Computational graphs contain **nodes** which can represent and input (tensor, matrix, vector, scalar) or **operation** that can be the input to another node. The nodes are connected by **edges**, which represent a function argument, they are pointers to nodes. Note that the computation graphs are directed and acyclic."
      ],
      "metadata": {
        "id": "s0EG6DhnsnTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the expression by setting our input variables as follows: $a=2$ and $b=1$. This will allow us to compute nodes up through the graph as shown in the computational graph above.  \n",
        "\n",
        "Rather than doing this by hand, we can use the automatic differentation engine provided by PyTorch.\n",
        "\n",
        "Let's import PyTorch first:"
      ],
      "metadata": {
        "id": "m9VvF4CVtW0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuD6zdWZp7DP"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the inputs like this:"
      ],
      "metadata": {
        "id": "b7EKlMrouClt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)"
      ],
      "metadata": {
        "id": "OZ2pB2A3uEQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we used `requires_grad=True` to tell the autograd engine that every operation on them should be tracked.\n",
        "\n",
        "These are the operations in code:"
      ],
      "metadata": {
        "id": "Zm6Xl05quGZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = a + b\n",
        "d = b + 1\n",
        "e = c * d\n",
        "\n",
        "# grads populated for non-leaf nodes\n",
        "c.retain_grad()\n",
        "d.retain_grad()\n",
        "e.retain_grad()"
      ],
      "metadata": {
        "id": "XwXomBUxu1Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we used `.retain_grad()` to allow gradients to be stored for non-leaf nodes as we are interested in inpecting those as well.\n",
        "\n",
        "Now that we have our computational graph, we can check the result when evaluating the expression:"
      ],
      "metadata": {
        "id": "UzCLJvMku46r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t-uhE6vvH2j",
        "outputId": "e834dbd0-0d8b-4123-d8fe-b9192aeaba9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a tensor with the value of `6.`, which verifies the results here:\n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png)"
      ],
      "metadata": {
        "id": "5eWub17iwi2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivatives on Computational Graphs\n",
        "\n",
        "Using the concept of computational graphs we are now interested in evaluating the **partial derivatives** of the edges of the graph. This will help in gathering the gradients of the graph. Remember that gradients are what we use to train the neural network and those calculations can be taken care of by the automatic differentation engine.\n",
        "\n",
        "The intuition is: we want to know, for example, if $a$ directly affects $c$, how does it affect it. In other words, if we change $a$ a little, how does $c$ change. This is referred to as the partial derivative of $c$ with respect to $a$.\n",
        "\n",
        "You can work this by hand, but the easy way to do this with PyTorch is by calling `.backward()` on $e$ and let the engine figure out the values. The `.backward()` signals the autograd engine to calculate the gradients and store them in the respective tensors’ `.grad` attribute."
      ],
      "metadata": {
        "id": "tjX3LCRmw22a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e.backward()"
      ],
      "metadata": {
        "id": "Nc6lnO5yy1Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PyTorch, we can do this by calling `a.grad`:"
      ],
      "metadata": {
        "id": "NvQcK9LTzD34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NWnWDg4zHDn",
        "outputId": "40cfe57c-23ee-4142-e62f-f7ef4b65fff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to understand the intuition behind this:\n",
        "\n",
        ">Let’s consider how $e$ is affected by $a$. If we change $a$ at a speed of 1, $c$ also changes at a speed of $1$. In turn, $c$ changing at a speed of $1$ causes $e$ to change at a speed of $2$. So $e$ changes at a rate of $1*2$ with respect to $a$.\n"
      ],
      "metadata": {
        "id": "c05nEObzzbPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, by hand this would be:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial \\boldsymbol{a}}=\\frac{\\partial e}{\\partial \\boldsymbol{c}} \\frac{\\partial \\boldsymbol{c}}{\\partial \\boldsymbol{a}} = 2 * 1\n",
        "$$"
      ],
      "metadata": {
        "id": "8xXLOU37BYOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since $a$ is not directly connectected to $e$, we can use some special rule which allows to sum over all paths from one node to the other in the computational graph and mulitplying the derivatives on each edge of the path together.\n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png)"
      ],
      "metadata": {
        "id": "A2iNJu6jzT5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q11abV90d6i",
        "outputId": "11571cdc-7e55-43a9-931f-ec1ecf140efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you work it out by hand, you are basically doing the following:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial b}=1 * 2+1 * 3\n",
        "$$\n",
        "\n",
        "It indicates how $b$ affects $e$ through $c$ and $d$. We are essentially summing over paths in the computational graph."
      ],
      "metadata": {
        "id": "2mGP1_iw0_ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are all the gradients collected, including non-leaf nodes:"
      ],
      "metadata": {
        "id": "sbJvhj5m13Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.grad, b.grad, c.grad, d.grad, e.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrUxwsrd3-f-",
        "outputId": "cc63c914-b2e4-43b9-8c43-dcd70975e8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.]) tensor([5.]) tensor([2.]) tensor([3.]) tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the computational graph above to verify that everything is correct. This is the power of computational graphs and how they are used by automatic differentation engines. It's also a very useful concept to understand when developing neural networks architectures and their correctness."
      ],
      "metadata": {
        "id": "HftIH5Mx4Pdj"
      }
    }
  ]
}