{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  An MLOps Pipeline With Training, Model Registry, and Batch Inference Harness SageMaker Pipelines With Batch Inference"
      ],
      "metadata": {
        "id": "7Axf-wpHUKJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "2RVg_GQgUeXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUhpaAk_T-Pp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from sagemaker import get_execution_role, session\n",
        "import pandas as pd\n",
        "from time import gmtime, strftime\n",
        "import sagemaker\n",
        "from sagemaker.model import Model\n",
        "from sagemaker.image_uris import retrieve\n",
        "from sagemaker.workflow.pipeline_context import PipelineSession\n",
        "from sagemaker.workflow.model_step import ModelStep\n",
        "from sagemaker.inputs import TrainingInput\n",
        "from sagemaker.workflow.steps import TrainingStep\n",
        "from sagemaker.workflow.parameters import ParameterString\n",
        "from sagemaker.estimator import Estimator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "region = boto3.Session().region_name\n",
        "sagemaker_session = sagemaker.Session()\n",
        "s3_prefix = 'xgboost-example'\n",
        "role = sagemaker.get_execution_role()\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "print(\"RoleArn: {}\".format(role))\n",
        "from sagemaker.workflow.pipeline import Pipeline\n",
        "\n",
        "# We also instantiate a Pipeline Session which ensures none of our steps run standalone and are only conducted when the Pipeline is executed.\n",
        "\n",
        "pipeline_session = PipelineSession()"
      ],
      "metadata": {
        "id": "vLdOvre8Uo6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Dataset (Abalone Dataset)"
      ],
      "metadata": {
        "id": "THsHKe1qU05w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/uci_abalone/train_csv/abalone_dataset1_train.csv .\n",
        "!aws s3 cp abalone_dataset1_train.csv s3://{default_bucket}/xgboost-regression/train.csv\n",
        "training_path = 's3://{}/xgboost-regression/train.csv'.format(default_bucket)"
      ],
      "metadata": {
        "id": "Ay8cCWSnUwm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since weâ€™re working with Batch Inference we need to ensure that our dataset is in a format that is compliant with the SageMaker XGBoost algorithm. The SageMaker XGBoost algorithm expects for the target column to be removed from the test dataset. Hence, we drop this column and create a test set for our Batch Inference step and upload it to S3."
      ],
      "metadata": {
        "id": "lw8_ssNZVC72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test = pd.read_csv('abalone_dataset1_train.csv')\n",
        "test = test.iloc[: , 1:]\n",
        "test.to_csv('test.csv', index=False)\n",
        "\n",
        "#Create a sagemaker session to be able to upload data to s3\n",
        "import boto3\n",
        "import sagemaker\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "#Uploading test data to S3 bucket\n",
        "prefix = \"xgb-test-batch-abalone\"\n",
        "test_data_path = sagemaker_session.upload_data('test.csv', key_prefix=prefix + '/test')"
      ],
      "metadata": {
        "id": "jPihWb1JVF0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next we parameterize our Pipeline with the input data locations and hardware necessary for the Training and Batch Inference portions of the Pipeline."
      ],
      "metadata": {
        "id": "0p5fK5euVQDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Parameters\n",
        "\n",
        "training_input_param = ParameterString(\n",
        "    name = \"training_input\",\n",
        "    default_value=training_path,\n",
        ")\n",
        "\n",
        "test_data_param = ParameterString(\n",
        "    name = \"test_input\",\n",
        "    default_value=test_data_path,\n",
        ")\n",
        "\n",
        "training_instance_param = ParameterString(\n",
        "    name = \"training_instance\",\n",
        "    default_value = \"ml.c5.xlarge\")\n",
        "\n",
        "batch_transform_param = ParameterString(\n",
        "    name = \"batch_inference\",\n",
        "    default_value = \"ml.m5.xlarge\")"
      ],
      "metadata": {
        "id": "DMbeMgGGVQly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Retrieve the AWS provided container for XGBoost that will be utilized for training and inference."
      ],
      "metadata": {
        "id": "LiKMDjzAVegV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f's3://{default_bucket}/{s3_prefix}/xgb_model'\n",
        "\n",
        "image_uri = sagemaker.image_uris.retrieve(\n",
        "    framework=\"xgboost\",\n",
        "    region=region,\n",
        "    version=\"1.0-1\",\n",
        "    py_version=\"py3\",\n",
        "    instance_type=training_instance_param,\n",
        ")\n",
        "\n",
        "image_uri"
      ],
      "metadata": {
        "id": "dtnmL5a1VjdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Step\n",
        "\n",
        "- Build out the Training portion via SageMaker Pipelines. For training with the XGBoost algorithm we create an object that points towards the hardware we need for our Training Job as well as the necessary hyperparameters to solve a regression problem using XGBoost."
      ],
      "metadata": {
        "id": "XQWPxLLMVpxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_train = Estimator(\n",
        "    image_uri=image_uri,\n",
        "    instance_type=training_instance_param,\n",
        "    instance_count=1,\n",
        "    output_path=model_path,\n",
        "    sagemaker_session=pipeline_session,\n",
        "    role=role\n",
        ")\n",
        "\n",
        "xgb_train.set_hyperparameters(\n",
        "    objective=\"reg:linear\",\n",
        "    num_round=40,\n",
        "    max_depth=4,\n",
        "    eta=0.1,\n",
        "    gamma=3,\n",
        "    min_child_weight=5,\n",
        "    subsample=0.6,\n",
        "    silent=0,\n",
        ")"
      ],
      "metadata": {
        "id": "M7LbhYfuVxAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encapsulate this object in a Training Step within SageMaker Pipelines and point towards our parameter with the training data."
      ],
      "metadata": {
        "id": "MhGui0OnV150"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = xgb_train.fit(\n",
        "    inputs={\n",
        "        \"train\": TrainingInput(\n",
        "            s3_data=training_input_param,\n",
        "            content_type=\"text/csv\",\n",
        "        )\n",
        "    }\n",
        ")\n",
        "\n",
        "training_step = TrainingStep(\n",
        "    name=\"Training\",\n",
        "    step_args=train_args,\n",
        ")"
      ],
      "metadata": {
        "id": "deGyJLCuV5pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model & Register Model Steps\n",
        "  - Create and catalog a SageMaker Model object. First we want to define a SageMaker Model from the Training Job that we defined in the previous step of the Pipeline. To do so we point towards the model artifacts that the Training Step generates and create our Model object using a ModelStep.\n",
        ""
      ],
      "metadata": {
        "id": "0YEu6djjV9OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(\n",
        "    image_uri=image_uri,\n",
        "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "    role=role,\n",
        "    sagemaker_session=pipeline_session\n",
        ")\n",
        "\n",
        "# Step to create a SageMaker Model\n",
        "create_model_step = ModelStep(\n",
        "    name=\"CreateXGBoostModel\",\n",
        "    step_args=model.create(),\n",
        ")"
      ],
      "metadata": {
        "id": "oLkvSiMfWFCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Registry"
      ],
      "metadata": {
        "id": "icKbhtuBWikp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.step_collections import RegisterModel\n",
        "\n",
        "register_step = RegisterModel(\n",
        " name=\"AbaloneRegisterModel\",\n",
        " model=model,\n",
        " content_types=[\"text/csv\"],\n",
        " response_types=[\"text/csv\"],\n",
        " inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
        " transform_instances=[\"ml.m5.xlarge\"],\n",
        " model_package_group_name='batchgroup',\n",
        ")"
      ],
      "metadata": {
        "id": "Eu0T217xWkHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Transform Step\n",
        "\n",
        "  - Unlike SageMaker Real-Time Inference there is no REST Endpoint we are creating. With Batch Inference thereâ€™s a Transformer object that you can define that captures your created Model, the hardware for inference, and the acceptable data formats for the Transform Job."
      ],
      "metadata": {
        "id": "X45JiTqDWmZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(model_name=create_model_step.properties.ModelName,\n",
        "                          instance_count=1, instance_type=batch_transform_param,\n",
        "                          assemble_with=\"Line\", accept=\"text/csv\",\n",
        "                          sagemaker_session=PipelineSession())"
      ],
      "metadata": {
        "id": "_43F1vVTWpzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We then wrap this up in a Transform Step and point towards the paramter we defined with our test dataset.\n",
        "\n",
        "transform_step = TransformStep(\n",
        "    name=\"AbaloneTransform\",\n",
        "    step_args=transformer.transform(data=test_data_param,\n",
        "                                    content_type = \"text/csv\"),\n",
        ")"
      ],
      "metadata": {
        "id": "wLqqB0hEW1co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Execution"
      ],
      "metadata": {
        "id": "I95VzrqJW4Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(\n",
        "    name=\"batch-pipeline-abalone\",\n",
        "    steps=[training_step, create_model_step, register_step, transform_step],\n",
        "    parameters= [training_input_param, training_instance_param, test_data_param, batch_transform_param]\n",
        ")\n",
        "# We can then execute the Pipeline with the following code, this specific pipeline execution should take about five minutes.\n",
        "\n",
        "pipeline.upsert(role_arn=role)\n",
        "execution = pipeline.start()\n",
        "execution.wait()"
      ],
      "metadata": {
        "id": "SOmuaFg0W51R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}